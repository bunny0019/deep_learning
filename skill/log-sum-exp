
log-sum-exp 是一种数值稳定性技巧，通常用于处理具有较大或较小值的数字，以防止数值溢出或下溢的问题。这个技巧在深度学习中经常用于计算损失函数或概率分布的对数。

假设我们有一组实数值 [x1, x2, x3, ..., xn]，我们想要计算它们的对数和：


log_sum_exp(x1, x2, x3, ..., xn) = log(e^x1 + e^x2 + e^x3 + ... + e^xn)
直接计算这个表达式可能会面临数值上溢或下溢的问题，因为指数函数 e^x 可能会产生非常大或非常小的值。

log-sum-exp 技巧通过引入一个参数 M 来解决这个问题，其中 M 是输入中的最大值。这个技巧的步骤如下：
找到输入中的最大值： M = max(x1, x2, x3, ..., xn)
计算偏移后的指数函数： e^(xi - M)，其中 i 是输入的索引。
计算偏移后的指数函数之和： sum_exp = e^(x1 - M) + e^(x2 - M) + e^(x3 - M) + ... + e^(xn - M)
最后，计算对数和： log_sum_exp(x1, x2, x3, ..., xn) = log(sum_exp) + M

这个技巧的关键在于，通过减去最大值 M，我们可以确保指数函数的输入在相对较小的范围内，从而避免了数值上溢或下溢的问题。同时，通过添加 M 到对数和中，我们可以得到原始输入的对数和。
在深度学习中，log-sum-exp 技巧通常在计算损失函数中的概率分布的对数时使用，以确保数值稳定性。这在二分类问题中的交叉熵损失函数中尤其常见，其中需要计算预测概率和真实标签之间的对数差。
